# Hive version: 3.1.2
# Haddop version: 3.3.1
# Aim: Perform operations on cpu log files and also do visualization of the result.
# Operations:
Display users and their record counts

Finding users with highest number of average hours

Finding users with lowest number of average hours

Finding users with highest numbers of idle hours
# Step1: start Hadoop
start-all.sh
# Step2: In python program created spark dataframe and load data from csv files.
# step3: Create hive table using spark sql
# Step4: Wrote queries for operations using hive queries
# Step5: Visualize the output of every operation 
